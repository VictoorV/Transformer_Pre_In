{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import copy\nimport numpy as np\nimport math\nimport torch\n\n######################################################################################################## Avoir plusieurs modudes avec des références différentes\ndef clones(module, n):\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n\n######################################################################################################## Scaled Dot Product Attention\ndef ScaledDotProductAttention(Q, K, V, mask = None):\n  \"\"\"\n  Q : Query head ; shape : batch x n_head x n_token x d_head\n  K : Key head ; shape : batch x n_head x n_token x d_head\n  V : Value head ; shape : batch x n_head x n_token x d_head\n  \"\"\"\n  d_head = Q.size(-1)\n  scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(d_head) # shape : batch * n_head * n_token * n_token\n  if mask is not None:\n    scores = scores.masked_fill_(mask==0, -1e4)\n\n  weights = scores.softmax(-1)\n  return torch.matmul(weights, V), weights\n\n######################################################################################################## Multi-Head Attention\nclass Multi_Head_Attention(torch.nn.Module):\n  def __init__(self, n_head, d_model, dropout=0.1):\n    super(Multi_Head_Attention, self).__init__()\n    assert d_model % n_head == 0\n    self.n_head = n_head\n    self.d_head = d_model // n_head\n    self.layers = clones(torch.nn.Linear(d_model, d_model), 4)\n    self.weights = None\n    self.dropout = torch.nn.Dropout(dropout)\n\n  def forward(self, Q, K, V, mask=None):\n    \"\"\"\n    Q : Query embedding ; shape : batch x n_token x d_model\n    K : Key embedding ; shape : batch x n_token x d_model\n    V : Value embedding ; shape : batch x n_token x d_model\n    \"\"\"\n    if mask is not None:\n      mask = mask.unsqueeze(1)\n\n    batch_size = Q.size(0)\n    Q, K, V = [l(x).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2) for l, x in zip(self.layers, (Q, K, V))] # batch x n_head x n_token x d_head\n    x, self.weights = ScaledDotProductAttention(Q, K, V, mask=mask) # batch x n_head x n_token x d_head\n    x = x.transpose(1,2).contiguous().view(batch_size, -1, self.n_head * self.d_head) # batch x n_token x d_model\n    return self.layers[-1](self.dropout(x))\n\n######################################################################################################## Double FFN\nclass FFN(torch.nn.Module):\n  def __init__(self, d_model, d_ffn, dropout=0.1):\n    super(FFN, self).__init__()\n    self.layer1 = torch.nn.Linear(d_model, d_ffn)\n    self.layer2 = torch.nn.Linear(d_ffn, d_model)\n    self.dropout = torch.nn.Dropout(dropout)\n\n  def forward(self, x):\n    return self.layer2(self.dropout(torch.nn.functional.silu(self.layer1(x))))\n\n######################################################################################################## Addition et normalisation pre-in\nclass PreNormResidual(torch.nn.Module):\n  def __init__(self, d_model, dropout=0.1):\n    super(PreNormResidual, self).__init__()\n    self.norm = torch.nn.LayerNorm(d_model)\n    self.dropout = torch.nn.Dropout(dropout)\n\n  def forward(self, x, sublayer):\n    return x + self.dropout(sublayer(self.norm(x)))\n\n######################################################################################################## Encoder block\nclass EncoderBlock(torch.nn.Module):\n  def __init__(self, n_head, d_model, d_ffn):\n    super(EncoderBlock, self).__init__()\n    self.MHA = Multi_Head_Attention(n_head, d_model)\n    self.Norm_And_Add = clones(PreNormResidual(d_model), 2)\n    self.FFN = FFN(d_model, d_ffn)\n\n  def forward(self, x, source_mask):\n    \"\"\"\n    x : Input ; shape : batch x n_token x d_model\n    source_mask : Mask for source sentence (padding tokens are masked) ; shape : batch x n_token x n_token\n    \"\"\"\n    x = self.Norm_And_Add[0](x, lambda x: self.MHA(x, x, x, mask=source_mask))\n    return self.Norm_And_Add[1](x, self.FFN)\n\n######################################################################################################## Encoder\nclass Encoder(torch.nn.Module):\n  def __init__(self, n_head, d_model, d_ffn, N):\n    super(Encoder, self).__init__()\n    self.layers = clones(EncoderBlock(n_head, d_model, d_ffn), N)\n    self.norm = torch.nn.LayerNorm(d_model)\n\n  def forward(self, x, source_mask):\n    for layer in self.layers:\n      x = layer(x, source_mask)\n    return self.norm(x)\n\n######################################################################################################## Decoder block\nclass DecoderBlock(torch.nn.Module):\n  def __init__(self, n_head, d_model, d_ffn):\n    super(DecoderBlock, self).__init__()\n    self.MHA_Masked = Multi_Head_Attention(n_head, d_model)\n    self.MHA_Encoder = Multi_Head_Attention(n_head, d_model)\n    self.Norm_And_Add = clones(PreNormResidual(d_model), 3)\n    self.FFN = FFN(d_model, d_ffn)\n\n  def forward(self, x, encoder_output, cross_mask, target_mask):\n    \"\"\"\n    x : Output ; shape : batch x n_token x d_model\n    encoder_output : Encoder final states ; shape : batch x n_token x d_model\n    cross_mask : Mask for cross attention (padding tokens are masked) ; shape : batch x n_token x n_token\n    target_mask : Mask for target sentence (future and padding tokens are masked) ; shape : batch x n_token x n_token\n    \"\"\"\n    x = self.Norm_And_Add[0](x, lambda x: self.MHA_Masked(x, x, x, mask=target_mask))\n    x = self.Norm_And_Add[1](x, lambda x: self.MHA_Encoder(x, encoder_output, encoder_output, mask=cross_mask))\n    return self.Norm_And_Add[2](x, self.FFN)\n\n######################################################################################################## Decoder\nclass Decoder(torch.nn.Module):\n  def __init__(self, target_vocab_size, n_head, d_model, d_ffn, N):\n    super(Decoder, self).__init__()\n    self.layers = clones(DecoderBlock(n_head, d_model, d_ffn), N)\n    self.norm = torch.nn.LayerNorm(d_model)\n    self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n\n  def forward(self, x, encoder_output, cross_mask, target_mask):\n    for layer in self.layers:\n      x = layer(x, encoder_output, cross_mask, target_mask)\n    x = self.norm(x)\n    return torch.nn.functional.log_softmax(self.final_layer(x), dim=-1)\n\n######################################################################################################## Encoder + Decoder\nclass EncoderDecoder(torch.nn.Module):\n  def __init__(self, target_vocab_size, n_head, d_model, d_ffn, N):\n    super(EncoderDecoder, self).__init__()\n    self.encoder = Encoder(n_head, d_model, d_ffn, N)\n    self.decoder = Decoder(target_vocab_size, n_head, d_model, d_ffn, N)\n\n  def forward(self, source, target, source_mask, target_mask, cross_mask):\n    encoder_output = self.encoder(source, source_mask)\n    return self.decoder(target, encoder_output, cross_mask, target_mask)\n\n######################################################################################################## Embedding\nclass Embeddings(torch.nn.Module):\n  def __init__(self, voc_size, d_model):\n    super(Embeddings, self).__init__()\n    self.embedding = torch.nn.Embedding(voc_size, d_model)\n    self.d_model = d_model\n\n  def forward(self, x):\n    \"\"\"\n    x : Input (int) ; shape : batch * n_token\n    \"\"\"\n    return self.embedding(x) * math.sqrt(self.d_model)\n\n######################################################################################################## Positional encoding\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n######################################################################################################## Build Transformer\nclass Transformer(torch.nn.Module):\n  def __init__(self, source_vocab_size, target_vocab_size, n_head, d_model, d_ffn, N):\n    super(Transformer, self).__init__()\n    self.source_embedding = Embeddings(source_vocab_size, d_model)\n    self.target_embedding = Embeddings(target_vocab_size, d_model)\n    self.PE = PositionalEncoding(d_model)\n    self.encoder_decoder = EncoderDecoder(target_vocab_size, n_head, d_model, d_ffn, N)\n    for p in self.parameters():\n            if p.dim() > 1:\n                torch.nn.init.xavier_uniform_(p)\n\n  def forward(self, source, target, source_mask, target_mask, cross_mask):\n    \"\"\"\n    source : Input (int) ; shape : batch * n_token\n    target : Output (int) ; shape : batch * n_token\n    \"\"\"\n    source = self.PE(self.source_embedding(source))\n    target = self.PE(self.target_embedding(target))\n    output = self.encoder_decoder(source, target, source_mask, target_mask, cross_mask)\n    return output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:26:35.788724Z","iopub.execute_input":"2025-03-15T17:26:35.789082Z","iopub.status.idle":"2025-03-15T17:26:42.076079Z","shell.execute_reply.started":"2025-03-15T17:26:35.789050Z","shell.execute_reply":"2025-03-15T17:26:42.075281Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def get_key_from_id(voc, id):\n  key = next((k for k, v in voc.items() if v == id), None)\n  return key","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:26:42.077209Z","iopub.execute_input":"2025-03-15T17:26:42.077633Z","iopub.status.idle":"2025-03-15T17:26:42.081531Z","shell.execute_reply.started":"2025-03-15T17:26:42.077605Z","shell.execute_reply":"2025-03-15T17:26:42.080698Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nimport pandas as pd\ndf = pd.read_csv(\"hf://datasets/FrancophonIA/english_french/train.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:26:42.083289Z","iopub.execute_input":"2025-03-15T17:26:42.083616Z","iopub.status.idle":"2025-03-15T17:26:49.971600Z","shell.execute_reply.started":"2025-03-15T17:26:42.083583Z","shell.execute_reply":"2025-03-15T17:26:49.970922Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"source_voc = {}\nsource_voc[\"[PAD]\"] = 0; source_voc[\"[UNK]\"] = 1; source_voc[\"[START]\"] = 2; source_voc[\"[END]\"] = 3\ntarget_voc = {}\ntarget_voc[\"[PAD]\"] = 0; target_voc[\"[UNK]\"] = 1; target_voc[\"[START]\"] = 2; target_voc[\"[END]\"] = 3\n\ndef source_tokenize_get_voc(row):\n  source_tokens = nltk.word_tokenize(row['french'].lower(), language='french')\n  for token in source_tokens:\n    if token not in source_voc:\n      source_voc[token] = len(source_voc)\n  return source_tokens\n\ndef target_tokenize_get_voc(row):\n  target_tokens = nltk.word_tokenize(row['english'].lower())\n  for token in target_tokens:\n    if token not in target_voc:\n      target_voc[token] = len(target_voc)\n  return [\"[START]\"] + target_tokens + [\"[END]\"]\n\ndf['tokenized_fr'] = df.apply(source_tokenize_get_voc, axis=1)\nprint('Done')\ndf['tokenized_en'] = df.apply(target_tokenize_get_voc, axis=1)\nprint('Done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:26:49.972725Z","iopub.execute_input":"2025-03-15T17:26:49.972996Z","iopub.status.idle":"2025-03-15T17:27:58.274188Z","shell.execute_reply.started":"2025-03-15T17:26:49.972975Z","shell.execute_reply":"2025-03-15T17:27:58.273265Z"}},"outputs":[{"name":"stdout","text":"Done\nDone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def source_seq_to_int(row, max_length=100):\n  sequence_copy = copy.deepcopy(row['tokenized_fr'])\n  if max_length is not None:\n    sequence_copy = sequence_copy[:max_length]\n  sequence_copy = [source_voc.get(word, source_voc[\"[UNK]\"]) for word in sequence_copy]\n  return sequence_copy\n\ndef target_seq_to_int(row, max_length=100):\n  sequence_copy = copy.deepcopy(row['tokenized_en'])\n  if max_length is not None:\n    sequence_copy = sequence_copy[:max_length]\n\n  sequence_copy = [target_voc.get(word, target_voc[\"[UNK]\"]) for word in sequence_copy]\n  return sequence_copy\n\ndf['int_fr'] = df.apply(source_seq_to_int, axis=1)\nprint('Done')\ndf['int_en'] = df.apply(target_seq_to_int, axis=1)\nprint('Done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:27:58.275206Z","iopub.execute_input":"2025-03-15T17:27:58.275524Z","iopub.status.idle":"2025-03-15T17:28:08.381615Z","shell.execute_reply.started":"2025-03-15T17:27:58.275493Z","shell.execute_reply":"2025-03-15T17:28:08.380839Z"}},"outputs":[{"name":"stdout","text":"Done\nDone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df = df[['french', 'tokenized_fr', 'int_fr', 'english', 'tokenized_en', 'int_en']]\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:28:08.382974Z","iopub.execute_input":"2025-03-15T17:28:08.383241Z","iopub.status.idle":"2025-03-15T17:28:08.500240Z","shell.execute_reply.started":"2025-03-15T17:28:08.383218Z","shell.execute_reply":"2025-03-15T17:28:08.499327Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   french  \\\n0             elle conduisait la voiture noire brillante.   \n1       chine est jamais calme à l'automne, mais il es...   \n2                   N'es-tu pas surprise de me voir ici ?   \n3                               Elle divorça de son mari.   \n4       paris est parfois le gel en juin, mais il est ...   \n...                                                   ...   \n320157  chine est calme au mois de novembre, et il est...   \n320158  pensez - vous que la traduction entre est diff...   \n320159  Pouvez-vous supporter la manière avec laquelle...   \n320160                     il va en inde en mai prochain.   \n320161               Je m'exerce chaque fois que je peux.   \n\n                                             tokenized_fr  \\\n0       [elle, conduisait, la, voiture, noire, brillan...   \n1       [chine, est, jamais, calme, à, l'automne, ,, m...   \n2          [n'es-tu, pas, surprise, de, me, voir, ici, ?]   \n3                       [elle, divorça, de, son, mari, .]   \n4       [paris, est, parfois, le, gel, en, juin, ,, ma...   \n...                                                   ...   \n320157  [chine, est, calme, au, mois, de, novembre, ,,...   \n320158  [pensez, -, vous, que, la, traduction, entre, ...   \n320159  [pouvez-vous, supporter, la, manière, avec, la...   \n320160           [il, va, en, inde, en, mai, prochain, .]   \n320161     [je, m'exerce, chaque, fois, que, je, peux, .]   \n\n                                                   int_fr  \\\n0                                  [4, 5, 6, 7, 8, 9, 10]   \n1       [11, 12, 13, 14, 15, 16, 17, 18, 19, 12, 20, 2...   \n2                        [24, 25, 26, 27, 28, 29, 30, 31]   \n3                                 [4, 32, 27, 33, 34, 10]   \n4       [35, 12, 20, 36, 37, 22, 23, 17, 18, 19, 12, 3...   \n...                                                   ...   \n320157  [11, 12, 14, 109, 116, 27, 200, 17, 44, 19, 12...   \n320158  [2860, 406, 88, 223, 6, 1154, 298, 12, 1068, 1...   \n320159  [1935, 536, 6, 938, 206, 1041, 19, 241, 8164, 31]   \n320160                 [19, 700, 22, 42, 22, 99, 814, 10]   \n320161         [111, 31391, 867, 1396, 223, 111, 407, 10]   \n\n                                                  english  \\\n0                    she was driving the shiny black car.   \n1       china is never quiet during autumn, but it is ...   \n2                    Aren't you surprised to see me here?   \n3                               She divorced her husband.   \n4       paris is sometimes freezing during june, but i...   \n...                                                   ...   \n320157  china is quiet during november, and it is neve...   \n320158  do you think translating between chinese and e...   \n320159            Can you put up with the way he behaves?   \n320160                     he is going to india next may.   \n320161                         I work out whenever I can.   \n\n                                             tokenized_en  \\\n0       [[START], she, was, driving, the, shiny, black...   \n1       [[START], china, is, never, quiet, during, aut...   \n2       [[START], are, n't, you, surprised, to, see, m...   \n3        [[START], she, divorced, her, husband, ., [END]]   \n4       [[START], paris, is, sometimes, freezing, duri...   \n...                                                   ...   \n320157  [[START], china, is, quiet, during, november, ...   \n320158  [[START], do, you, think, translating, between...   \n320159  [[START], can, you, put, up, with, the, way, h...   \n320160  [[START], he, is, going, to, india, next, may,...   \n320161  [[START], i, work, out, whenever, i, can, ., [...   \n\n                                                   int_en  \n0                        [2, 4, 5, 6, 7, 8, 9, 10, 11, 3]  \n1       [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 13, 21...  \n2              [2, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3]  \n3                               [2, 4, 34, 35, 36, 11, 3]  \n4       [2, 37, 13, 21, 38, 16, 24, 18, 19, 20, 13, 39...  \n...                                                   ...  \n320157  [2, 12, 13, 15, 16, 185, 18, 43, 20, 13, 14, 3...  \n320158  [2, 164, 27, 104, 902, 273, 1232, 43, 192, 13,...  \n320159  [2, 672, 27, 146, 422, 189, 7, 578, 63, 6462, ...  \n320160          [2, 63, 13, 369, 29, 41, 664, 103, 11, 3]  \n320161              [2, 74, 225, 88, 711, 74, 672, 11, 3]  \n\n[320162 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>french</th>\n      <th>tokenized_fr</th>\n      <th>int_fr</th>\n      <th>english</th>\n      <th>tokenized_en</th>\n      <th>int_en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>elle conduisait la voiture noire brillante.</td>\n      <td>[elle, conduisait, la, voiture, noire, brillan...</td>\n      <td>[4, 5, 6, 7, 8, 9, 10]</td>\n      <td>she was driving the shiny black car.</td>\n      <td>[[START], she, was, driving, the, shiny, black...</td>\n      <td>[2, 4, 5, 6, 7, 8, 9, 10, 11, 3]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>chine est jamais calme à l'automne, mais il es...</td>\n      <td>[chine, est, jamais, calme, à, l'automne, ,, m...</td>\n      <td>[11, 12, 13, 14, 15, 16, 17, 18, 19, 12, 20, 2...</td>\n      <td>china is never quiet during autumn, but it is ...</td>\n      <td>[[START], china, is, never, quiet, during, aut...</td>\n      <td>[2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 13, 21...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>N'es-tu pas surprise de me voir ici ?</td>\n      <td>[n'es-tu, pas, surprise, de, me, voir, ici, ?]</td>\n      <td>[24, 25, 26, 27, 28, 29, 30, 31]</td>\n      <td>Aren't you surprised to see me here?</td>\n      <td>[[START], are, n't, you, surprised, to, see, m...</td>\n      <td>[2, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Elle divorça de son mari.</td>\n      <td>[elle, divorça, de, son, mari, .]</td>\n      <td>[4, 32, 27, 33, 34, 10]</td>\n      <td>She divorced her husband.</td>\n      <td>[[START], she, divorced, her, husband, ., [END]]</td>\n      <td>[2, 4, 34, 35, 36, 11, 3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>paris est parfois le gel en juin, mais il est ...</td>\n      <td>[paris, est, parfois, le, gel, en, juin, ,, ma...</td>\n      <td>[35, 12, 20, 36, 37, 22, 23, 17, 18, 19, 12, 3...</td>\n      <td>paris is sometimes freezing during june, but i...</td>\n      <td>[[START], paris, is, sometimes, freezing, duri...</td>\n      <td>[2, 37, 13, 21, 38, 16, 24, 18, 19, 20, 13, 39...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>320157</th>\n      <td>chine est calme au mois de novembre, et il est...</td>\n      <td>[chine, est, calme, au, mois, de, novembre, ,,...</td>\n      <td>[11, 12, 14, 109, 116, 27, 200, 17, 44, 19, 12...</td>\n      <td>china is quiet during november, and it is neve...</td>\n      <td>[[START], china, is, quiet, during, november, ...</td>\n      <td>[2, 12, 13, 15, 16, 185, 18, 43, 20, 13, 14, 3...</td>\n    </tr>\n    <tr>\n      <th>320158</th>\n      <td>pensez - vous que la traduction entre est diff...</td>\n      <td>[pensez, -, vous, que, la, traduction, entre, ...</td>\n      <td>[2860, 406, 88, 223, 6, 1154, 298, 12, 1068, 1...</td>\n      <td>do you think translating between chinese and e...</td>\n      <td>[[START], do, you, think, translating, between...</td>\n      <td>[2, 164, 27, 104, 902, 273, 1232, 43, 192, 13,...</td>\n    </tr>\n    <tr>\n      <th>320159</th>\n      <td>Pouvez-vous supporter la manière avec laquelle...</td>\n      <td>[pouvez-vous, supporter, la, manière, avec, la...</td>\n      <td>[1935, 536, 6, 938, 206, 1041, 19, 241, 8164, 31]</td>\n      <td>Can you put up with the way he behaves?</td>\n      <td>[[START], can, you, put, up, with, the, way, h...</td>\n      <td>[2, 672, 27, 146, 422, 189, 7, 578, 63, 6462, ...</td>\n    </tr>\n    <tr>\n      <th>320160</th>\n      <td>il va en inde en mai prochain.</td>\n      <td>[il, va, en, inde, en, mai, prochain, .]</td>\n      <td>[19, 700, 22, 42, 22, 99, 814, 10]</td>\n      <td>he is going to india next may.</td>\n      <td>[[START], he, is, going, to, india, next, may,...</td>\n      <td>[2, 63, 13, 369, 29, 41, 664, 103, 11, 3]</td>\n    </tr>\n    <tr>\n      <th>320161</th>\n      <td>Je m'exerce chaque fois que je peux.</td>\n      <td>[je, m'exerce, chaque, fois, que, je, peux, .]</td>\n      <td>[111, 31391, 867, 1396, 223, 111, 407, 10]</td>\n      <td>I work out whenever I can.</td>\n      <td>[[START], i, work, out, whenever, i, can, ., [...</td>\n      <td>[2, 74, 225, 88, 711, 74, 672, 11, 3]</td>\n    </tr>\n  </tbody>\n</table>\n<p>320162 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, dataframe):\n        self.data = dataframe\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src_int = torch.tensor(self.data.iloc[idx]['int_fr'])\n        tgt_int = torch.tensor(self.data.iloc[idx]['int_en'])\n\n        return src_int, tgt_int\n\ndataset = TranslationDataset(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:28:08.501122Z","iopub.execute_input":"2025-03-15T17:28:08.501389Z","iopub.status.idle":"2025-03-15T17:28:08.506445Z","shell.execute_reply.started":"2025-03-15T17:28:08.501366Z","shell.execute_reply":"2025-03-15T17:28:08.505557Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def create_source_mask(source_tensor):\n  mask = source_tensor != 0\n  mask = mask.unsqueeze(1) * torch.ones(source_tensor.size(-1)).unsqueeze(1)\n  return mask.bool()\n\ndef create_target_mask(target_tensor):\n  mask = target_tensor != 0\n  mask = mask.unsqueeze(1) * torch.ones(target_tensor.size(-1)).unsqueeze(1)\n  mask = mask * torch.tril(torch.ones(mask.size(1), mask.size(1))).unsqueeze(0)\n  return mask.bool()\n\ndef create_cross_mask(source_tensor, target_tensor):\n  mask = source_tensor != 0\n  mask = mask.unsqueeze(1) * torch.ones(target_tensor.size(-1)).unsqueeze(1)\n  return mask.bool()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:28:08.508053Z","iopub.execute_input":"2025-03-15T17:28:08.508262Z","iopub.status.idle":"2025-03-15T17:28:08.531507Z","shell.execute_reply.started":"2025-03-15T17:28:08.508242Z","shell.execute_reply":"2025-03-15T17:28:08.530653Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  src_batch, tgt_batch = zip(*batch)\n  src_input = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True)\n  tgt_input = [t[:-1] for t in tgt_batch]\n  tgt_output = [t[1:] for t in tgt_batch]\n  tgt_input = torch.nn.utils.rnn.pad_sequence(tgt_input, batch_first=True)\n  tgt_output = torch.nn.utils.rnn.pad_sequence(tgt_output, batch_first=True)\n  return src_input.long(), tgt_input.long(), tgt_output.long(), create_source_mask(src_input), create_target_mask(tgt_input), create_cross_mask(src_input, tgt_input)\n\ndataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:28:08.532566Z","iopub.execute_input":"2025-03-15T17:28:08.532864Z","iopub.status.idle":"2025-03-15T17:28:08.550795Z","shell.execute_reply.started":"2025-03-15T17:28:08.532835Z","shell.execute_reply":"2025-03-15T17:28:08.550199Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import time\n\ndef train_model(model, loss_function, optimizer, data_loader, scheduler=None):\n    scaler = torch.amp.GradScaler()\n    start_time = time.time()\n    progress_interval = len(data_loader) // 3\n    \n    model.train()\n\n    current_loss = 0.0\n    counter = 0\n\n    for i, (src_input, tgt_input, tgt_output, src_mask, tgt_mask, cross_mask) in enumerate(data_loader):\n        optimizer.zero_grad()\n        \n        src_input = src_input.to(device, non_blocking=True)\n        tgt_input = tgt_input.to(device, non_blocking=True)\n        tgt_output = tgt_output.to(device, non_blocking=True)\n        src_mask = src_mask.to(device, non_blocking=True)\n        tgt_mask = tgt_mask.to(device, non_blocking=True)\n        cross_mask = cross_mask.to(device, non_blocking=True)\n\n        with torch.amp.autocast(device_type='cuda:0', dtype=torch.float16): \n            out = model(src_input, tgt_input, src_mask, tgt_mask, cross_mask)\n\n            loss = loss_function(out.contiguous().view(-1, out.size(-1)),\n                                tgt_output.contiguous().view(-1))\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n        \n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        \n        current_loss += loss\n        counter += 1\n\n        if (i + 1) % progress_interval == 0 or (i + 1) == len(data_loader):\n            percent_complete = (i + 1) / len(data_loader) * 100\n            print(f\"Progress: {percent_complete:.1f}% - Batch: {i + 1}/{len(data_loader)}; Loss: {current_loss / counter:.6f}\")\n            print(f\"Lr: {scheduler.get_last_lr()[0]:.6f}\")\n            current_loss = 0.0\n            counter = 0\n            \n    end_time = time.time()  # Arrête le chronomètre\n    elapsed_time = end_time - start_time\n    print(f\"Terminé en {elapsed_time:.2f} secondes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T17:28:08.551574Z","iopub.execute_input":"2025-03-15T17:28:08.551864Z","iopub.status.idle":"2025-03-15T17:28:08.571005Z","shell.execute_reply.started":"2025-03-15T17:28:08.551835Z","shell.execute_reply":"2025-03-15T17:28:08.570216Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"if __name__ == '__main__':\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    n_epoch = 12\n    \n    model = Transformer(len(source_voc), len(target_voc), 6, 360, 1440, 4)\n    model = torch.nn.DataParallel(model)\n    model = model.to(device)\n\n    loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, weight_decay=0.1)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(dataloader), epochs=n_epoch)\n\n    for i in range(n_epoch):\n        print(\"Epoch :\", i+1)\n        train_model(model, loss_function, optimizer, dataloader, scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T18:24:44.954654Z","iopub.execute_input":"2025-03-15T18:24:44.955030Z","iopub.status.idle":"2025-03-15T19:01:32.164622Z","shell.execute_reply.started":"2025-03-15T18:24:44.955002Z","shell.execute_reply":"2025-03-15T19:01:32.163677Z"}},"outputs":[{"name":"stdout","text":"Epoch : 1\nProgress: 33.3% - Batch: 417/1251; Loss: 5.579123\nLr: 0.000060\nProgress: 66.7% - Batch: 834/1251; Loss: 2.491803\nLr: 0.000119\nProgress: 100.0% - Batch: 1251/1251; Loss: 1.527998\nLr: 0.000212\nTerminé en 184.72 secondes\nEpoch : 2\nProgress: 33.3% - Batch: 417/1251; Loss: 1.054403\nLr: 0.000330\nProgress: 66.7% - Batch: 834/1251; Loss: 0.832548\nLr: 0.000464\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.706036\nLr: 0.000604\nTerminé en 185.01 secondes\nEpoch : 3\nProgress: 33.3% - Batch: 417/1251; Loss: 0.578533\nLr: 0.000736\nProgress: 66.7% - Batch: 834/1251; Loss: 0.551993\nLr: 0.000850\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.527901\nLr: 0.000936\nTerminé en 185.22 secondes\nEpoch : 4\nProgress: 33.3% - Batch: 417/1251; Loss: 0.433877\nLr: 0.000987\nProgress: 66.7% - Batch: 834/1251; Loss: 0.432932\nLr: 0.001000\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.423294\nLr: 0.000994\nTerminé en 184.09 secondes\nEpoch : 5\nProgress: 33.3% - Batch: 417/1251; Loss: 0.341603\nLr: 0.000981\nProgress: 66.7% - Batch: 834/1251; Loss: 0.352461\nLr: 0.000961\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.345704\nLr: 0.000933\nTerminé en 183.27 secondes\nEpoch : 6\nProgress: 33.3% - Batch: 417/1251; Loss: 0.278733\nLr: 0.000898\nProgress: 66.7% - Batch: 834/1251; Loss: 0.290517\nLr: 0.000858\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.295301\nLr: 0.000812\nTerminé en 182.76 secondes\nEpoch : 7\nProgress: 33.3% - Batch: 417/1251; Loss: 0.231858\nLr: 0.000761\nProgress: 66.7% - Batch: 834/1251; Loss: 0.238739\nLr: 0.000706\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.242088\nLr: 0.000647\nTerminé en 183.53 secondes\nEpoch : 8\nProgress: 33.3% - Batch: 417/1251; Loss: 0.185143\nLr: 0.000587\nProgress: 66.7% - Batch: 834/1251; Loss: 0.194260\nLr: 0.000525\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.196607\nLr: 0.000462\nTerminé en 183.56 secondes\nEpoch : 9\nProgress: 33.3% - Batch: 417/1251; Loss: 0.149059\nLr: 0.000401\nProgress: 66.7% - Batch: 834/1251; Loss: 0.150695\nLr: 0.000341\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.154050\nLr: 0.000283\nTerminé en 183.43 secondes\nEpoch : 10\nProgress: 33.3% - Batch: 417/1251; Loss: 0.116726\nLr: 0.000229\nProgress: 66.7% - Batch: 834/1251; Loss: 0.119080\nLr: 0.000178\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.117982\nLr: 0.000133\nTerminé en 183.60 secondes\nEpoch : 11\nProgress: 33.3% - Batch: 417/1251; Loss: 0.095225\nLr: 0.000094\nProgress: 66.7% - Batch: 834/1251; Loss: 0.096206\nLr: 0.000061\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.095895\nLr: 0.000035\nTerminé en 182.74 secondes\nEpoch : 12\nProgress: 33.3% - Batch: 417/1251; Loss: 0.085180\nLr: 0.000015\nProgress: 66.7% - Batch: 834/1251; Loss: 0.084381\nLr: 0.000004\nProgress: 100.0% - Batch: 1251/1251; Loss: 0.084527\nLr: 0.000000\nTerminé en 183.88 secondes\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def get_input(sequence, voc):\n  sequence_copy = copy.deepcopy(sequence)\n  sequence_copy = [voc.get(word, source_voc[\"[UNK]\"]) for word in sequence_copy]\n  return sequence_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T19:01:34.587618Z","iopub.execute_input":"2025-03-15T19:01:34.587967Z","iopub.status.idle":"2025-03-15T19:01:34.592254Z","shell.execute_reply.started":"2025-03-15T19:01:34.587936Z","shell.execute_reply":"2025-03-15T19:01:34.591513Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def infer(sequences_fr, model, device):\n    batch_size = len(sequences_fr)\n    model.eval()\n    input_fr = []\n    src_mask = []\n    for seq in sequences_fr:\n        input_fr.append(torch.tensor([source_voc.get(word, source_voc[\"[UNK]\"]) for word in nltk.word_tokenize(seq[0].lower(), language='french')]).unsqueeze(0).long())\n        src_mask.append(create_source_mask(input_fr[-1]))\n    sequence_en = [[] for _ in range(batch_size)]\n    res = []\n    for i in range(batch_size):\n        next_token = 2\n        while next_token != 3:\n            sequence_en[i].append(next_token)\n            input_en = torch.tensor(sequence_en[i]).unsqueeze(0).long()\n            tgt_mask = create_target_mask(input_en)\n            cross_mask = create_cross_mask(input_fr[i], input_en)\n            log_logits = model(input_fr[i].to(device), input_en.to(device), src_mask[i].to(device), tgt_mask.to(device), cross_mask.to(device))\n            next_token = torch.argmax(log_logits[:, -1], dim=-1).item()\n        res.append([' '.join([get_key_from_id(target_voc, i) for i in sequence_en[i][1:]])])\n    return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T19:01:36.970240Z","iopub.execute_input":"2025-03-15T19:01:36.970523Z","iopub.status.idle":"2025-03-15T19:01:36.977352Z","shell.execute_reply.started":"2025-03-15T19:01:36.970501Z","shell.execute_reply":"2025-03-15T19:01:36.976507Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"seq_fr = [\n    [\"Il pleut beaucoup aujourd'hui, donc nous restons à la maison.\"],\n    [\"Je parle beaucoup mieux anglais que toi.\"],\n    [\"Nous avons mangé des crêpes pour le petit déjeuner.\"],\n    [\"Le chien court dans le jardin. Il est très rapide et il aime jouer avec la balle.\"],\n    [\"Je vais à l'école. J'aime jouer avec mes amis.\"],\n    [\"Nous allons au parc. Il y a beaucoup d’arbres et nous allons jouer au football.\"],\n    [\"Le ciel devient sombre, la nuit tombe. Il est l'heure de regarder un film à la télé.\"]\n]\ninfer(seq_fr, model, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T19:11:05.668105Z","iopub.execute_input":"2025-03-15T19:11:05.668395Z","iopub.status.idle":"2025-03-15T19:11:07.632378Z","shell.execute_reply.started":"2025-03-15T19:11:05.668373Z","shell.execute_reply":"2025-03-15T19:11:07.631501Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"[[\"it 's raining today , so we stay home .\"],\n ['i speak english a lot better than you do .'],\n ['we ate pancakes for breakfast .'],\n [\"the dog runs in the garden . he 's very fast and he likes to play with the ball .\"],\n ['i go to school . i like to play with my friends .'],\n ['we go to the park . there are a lot of trees and we are going to play soccer .'],\n [\"the sky is getting dark , the night is falling . it 's time to watch a movie on tv .\"]]"},"metadata":{}}],"execution_count":53}]}